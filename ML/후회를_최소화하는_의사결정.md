# 후회를 최소화하는 의사 결정

# Contents
- Reinforcement Learning
- 탐색-착취 딜레마 소개
- Multi-Armed Bandit
- Bayesian Optimization

# 탐색-착취 딜레마
Online decision making involves a fundamental choice.

```
오늘은 뭐 먹지?
```
여러가지 메뉴가 있고 한 메뉴씩 try 하면서 점수를 매겨나갈 때, 

점수를 그 다음 의사 결정에 반영할 것인가? 말것인가?

## 착취, 탐색.. 후회
- Exploitation: make the best decision given current information.
  ```
  착취: 내가 알고 있는 곳 중 최대(평균) 만족도를 주는 음식점을 계속 선택한다.
  ```
- Exploration: gather more information.
  ```
  탐색: 안가봤거나 최대 만족도를 주지 않았던 음식점을 선택한다.
  ```
- 후회(Regret): 가본 후, 진짜 최대 만족도를 주는 음식점을 선택하면 좋았을걸.. 하며 후회하는 것을 수치화.

## 탐색을 해야 하는 이유?
진짜 최대 만족도를 주는 음식점을 발견하지 못할 수도 있기 때문이다. 더 나은 솔루션을 찾기 위해 도전 한다.

## 이것이 딜레마인 이유?
- 탐색과 착취를 적절히 해야 하는데 실제 agent는 내가 최선의 것을 발견했는지 그 시점에 바로 알 수 없다.
- 언제 탐색을 멈춰야 할 지 알 수 없다.
- 이상적인 방법이 존재하지 않으므로 딜레마이다.
- 온라인 의사 결정 문제의 근본적 속성이다.

# MAB(Multi-Armed Bandit)
MAB는 <A,R>의 튜플이다.

A는 N 개의 가능한 action의 집합이다.

R(r|a)는 선택한 action에 대해 주어진 rewards의 unknown probability distribution 이다.

목표는 누적 reward를 극대화 하는 것이다.

## 예시, 카지노
- 카지노는 강도(bandit)
- 슬롯머신에는 팔(arm)이 있음
- 여러 개의 슬롯 머신을 카지노는 팔이 여러 개인 강도임.
- 슬롯머신마다 당첨 확률이 다른데, 나는 구체적인 확률 값을 모름.
- 이 때, 어떤 arm을 당겨볼 것인가?

탐색: 새로운 기계에서 시도 해 볼 것인가?

착취: 여채껏 제일 당첨 확률이 높았던 기계에서 계속 할 것인가?

## 대표적 MAB 전략들
### e–greedy(e: 입실론)
The e-greedy algorithm continues to explore forever.

1. e의 확률로 최대 평균 보상 arm을 당김
2. (1-e)의 확률로 나머지 arm 중에 임의로 당김

- 상수 e는 minimum regret임을 보증한다.
- 단순한 방법이지만 동작한다.
- A/B testing에서 사용 된다.
- Google Analytics가 동작하는 내부 방식이 (bayesian)MAB.

### Upper confidence bound
1. 각 arm의 평균 보상에 대한 confidence interval 집계.
2. Upper confidence bound를 극대화 하는 action을 선택한다(arm을 당긴다).

- Optimism in face of uncertainty
- Bayesian 접근법과 관련이 있음.

### Bayesian Optimizationan
Bayesian optimization is a powerful tool for optimizing objective functions which are very costly or slow to evaluate.

In particular, we consider problems where the maiximum is sought for an expensive fundtion.

A Model-based optimization method.

탐색이 필요한 구간을 지속적으로 찾아나가는 것.

```
Bulding a model from data              +      Decision making under uncertainty
(supervised/unsupervised learning)            (reinforcement learning)
```

### 목표
목적 함수가 최대(최소)가 되는 점을 최대한 빨리 찾는 것

Regret: 진짜 최댓값 - 현재까지 찾은 최댓값<br/>
(이 때, 진짜 최댓값은 미리 알고 있어야 한다.)


## 정리
- MAB를 이용하면 실험 과정 중에 regret을 줄일 수 있다.
- Bayesian Optimization을 이용하면 적은 탐색으로 함수를 최적화 할 수 있다.

## 참고
- http://home.deib.polimi.it/restelli/MyWebSite/pdf/rl5.pdf
- [Bayesian optimization for reinforcement learning](https://blog.sigopt.com/posts/using-bayesian-optimization-for-reinforcement-learning)
- https://static.sigopt.com/2d66b84dcdbbd7fffad087f58b67a585eb89444c/pdf/SigOpt_Bayesian_Optimization_Primer.pdf
