강의: http://www.edwith.org/others26/lecture/10731/

# Linear Regression
하나의 가설을 세워야 Regression 모델 학습이 가능하다.
가설(H, Hyphothesis)의 형태가 일차 방정식으로 표현 가능할때 Linear Regression이라 칭한다.
> H(x) = Wx + b

## Cost(Loss function)
우리가 세운 가설과 실제 데이터가 얼마나 다른지 계산하는 함수
cost(W,b)가 적을 수록 좋으며, minimize cost를 하기 위해 linear regression을 학습시킨다.

![Cost function](images/cost_function.png)


## Gradient Descent Algorithm
- cost minimization을 위한 알고리즘.
- cost 함수 그래프를 그리면 2차 포물선이 그려짐.
- cost의 최솟값은 미분을 통해 해결할 수 있다.
![Gradient_descent](images/gradient_descent.png)

## Multi-variable linear regression
Regression using three inputs(x1, x2, x3)

|x1|x2|x3|Y|
|--|--|--|--|
|73|80|75|152|
|93|88|93|185|
|89|91|90|180|
|96|98|100|196|
|73|66|70|142|

```
H(x1, x2, x3) = x1w1 + x2w2 + x3y3
```
